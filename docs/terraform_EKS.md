# End-to-End DevOps Project: AWS EKS with Terraform (Modular Approach)

## ğŸ“Œ Project Overview

This project demonstrates a **complete, real-world DevOps implementation** where:

- Infrastructure is provisioned on AWS using **Terraform (IaC)**
- Terraform is written using a **modular architecture**
- Remote backend with **S3 + DynamoDB state locking** is used
- An **Amazon EKS cluster** is created
- A **containerized application** is deployed on Kubernetes
- The application is exposed publicly via **AWS LoadBalancer**
- All common **Terraform + EKS issues** are identified and resolved

This project was built step by step for **deep understanding, hands-on learning, and interview readiness**.

---

## ğŸ§  Why Kubernetes & EKS?

### Why Kubernetes?
Kubernetes is used to:
- Run containers reliably
- Self-heal failed pods
- Scale applications
- Perform rolling updates
- Abstract infrastructure complexity

### Why Amazon EKS?
- Managed Kubernetes control plane
- High availability
- Secure and scalable
- Industry standard for production workloads on AWS

---

## ğŸ§  Why Terraform?

Terraform is used because:
- Infrastructure is defined as code
- Changes are version-controlled
- Infrastructure is reproducible
- No manual AWS console dependency
- Supports modular, reusable design

ğŸ“Œ **Interview one-liner**:  
> â€œTerraform enables declarative, version-controlled infrastructure provisioning.â€

---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ terraform/
â”‚ â”œâ”€â”€ backend/ # Backend infra (S3 + DynamoDB)
â”‚ â”œâ”€â”€ modules/
â”‚ â”‚ â”œâ”€â”€ vpc/ # Custom VPC module
â”‚ â”‚ â””â”€â”€ eks/ # Custom EKS module
â”‚ â”œâ”€â”€ backend.tf # Remote backend config
â”‚ â”œâ”€â”€ main.tf # Root module (orchestrator)
â”‚ â”œâ”€â”€ variables.tf
â”‚ â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ k8s/ # Kubernetes manifests
â”‚ â”œâ”€â”€ deployment.yaml
â”‚ â””â”€â”€ service.yaml
â”‚
â”œâ”€â”€ app/
â”œâ”€â”€ docker/
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸ§± Terraform Architecture (Very Important Concept)

Terraform is designed in **two layers**:

### Layer 1: Backend Infrastructure (Run Once)
Creates:
- S3 bucket for state
- DynamoDB table for locking

This layer uses **local state**.

### Layer 2: Main Infrastructure
Uses:
- Remote backend (S3 + DynamoDB)
- Modular Terraform code
- Creates VPC + EKS + Node Groups

ğŸ“Œ **Terraform backend must exist BEFORE it is used.**

---

## ğŸ” Terraform Backend (State & Locking)

### Why Remote Backend?
- Prevents state loss
- Enables collaboration
- Avoids concurrent applies
- Secures infrastructure metadata

### Backend Resources
- **S3** â†’ Stores `terraform.tfstate`
- **DynamoDB** â†’ State locking

### Backend Configuration (Root `backend.tf`)
```hcl
terraform {
  backend "s3" {
    bucket         = "terraform-eks-s3-bucket-ums"
    key            = "eks/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-state-locks-ums"
    encrypt        = true
  }
}

ğŸš« Terraform State & Git (Important Rule)

Terraform state files must NEVER be committed.

Root .gitignore
terraform/.terraform/
terraform/.terraform.lock.hcl
terraform/**/*.tfstate
terraform/**/*.tfstate.backup


ğŸ“Œ Interview rule:

â€œTerraform state should never be committed to Git.â€

ğŸ§© Modular Terraform Design (Core Concept)
Why Modules?

Reusability

Clean separation

Easier debugging

Scalable design

Modules Used
Module	Responsibility
VPC	Networking (subnets, NAT, routing)
EKS	Cluster + Node Groups
ğŸŒ VPC Module â€“ Key Concepts
What the VPC Module Creates

VPC

Public subnets

Private subnets

Internet Gateway

NAT Gateway

Route tables

EKS-required subnet tags

Important EKS Tags
"kubernetes.io/cluster/<cluster-name>" = "shared"
"kubernetes.io/role/elb" = "1"
"kubernetes.io/role/internal-elb" = "1"


ğŸ“Œ Without these tags, LoadBalancers do not work.

â˜¸ï¸ EKS Module â€“ Key Concepts
IAM Roles (Common Failure Point)

Two IAM roles are required:

EKS Cluster Role

EKS Node Group Role

Required Policies
Role	Policies
Cluster	AmazonEKSClusterPolicy, AmazonEKSServicePolicy
Node	WorkerNode, CNI, ECRReadOnly

Missing any policy can cause EKS creation to fail.

âŒ Problems Faced During EKS Creation (And Fixes)
âŒ Problem 1: EKS creation failed randomly

Cause:

Missing AmazonEKSServicePolicy

Fix:

Attach required IAM policy

âŒ Problem 2: Node Group failed to join cluster

Cause:

IAM policies not fully attached before node group creation

Fix:

Explicit depends_on for IAM attachments

âŒ Problem 3: Cluster created but nodes not ready

Cause:

No explicit security group

VPC ambiguity

Fix:

Create and attach cluster security group using vpc_id

âŒ Problem 4: Terraform backend confusion

Cause:

Trying to create backend and use backend in same config

Fix:

Separate backend infrastructure folder

Run backend first

ğŸ§  Root Terraform Module (Orchestration)

The root main.tf:

Configures provider

Uses remote backend

Invokes VPC module

Passes outputs to EKS module

This ensures correct dependency flow:

VPC â†’ EKS

ğŸš€ Deploying Application to EKS
Step 1: Configure kubectl
aws eks update-kubeconfig \
  --region us-east-1 \
  --name <cluster-name>

Step 2: Verify Nodes
kubectl get nodes

ğŸ“¦ Kubernetes Deployment
Deployment (deployment.yaml)

Defines desired pods

Uses Docker image from registry

Ensures high availability

Service (service.yaml)
type: LoadBalancer


This creates an AWS ELB automatically.

ğŸŒ Accessing the Application
kubectl get svc


Open in browser:

http://<EXTERNAL-IP>/health

ğŸ” Traffic Flow
Browser
 â†’ AWS ELB
 â†’ Kubernetes Service
 â†’ Pod
 â†’ Container

ğŸ§¹ Cleanup (Cost Safety)
kubectl delete -f k8s/
terraform destroy


ğŸ“Œ Always destroy unused infrastructure.

##########################################################

# Deploy Application to AWS EKS (Post-Terraform)

## ğŸ“Œ Overview

This document explains the **step-by-step procedure to deploy a containerized application to AWS EKS** after the infrastructure (VPC + EKS cluster + Node Groups) has been successfully created using **Terraform**.

This follows **real-world DevOps best practices** and is suitable for **future revision and interview discussion**.

---

## ğŸ§  Prerequisites

Ensure the following are already completed:

- âœ… AWS account configured
- âœ… EKS cluster created using Terraform
- âœ… Worker nodes are in `Ready` state
- âœ… Docker image pushed to Docker Hub
- âœ… `kubectl` and `aws` CLI installed locally
- âœ… Kubernetes manifests (`deployment.yaml`, `service.yaml`) available

---

## ğŸ“‚ Repository Structure (Relevant)

â”œâ”€â”€ terraform/ # Terraform IaC (EKS already created)
â”œâ”€â”€ k8s/ # Kubernetes manifests
â”‚ â”œâ”€â”€ deployment.yaml
â”‚ â””â”€â”€ service.yaml
â”œâ”€â”€ app/
â”œâ”€â”€ docker/
â””â”€â”€ README.md


---

## ğŸš€ Step 1: Connect kubectl to EKS Cluster

Terraform creates the EKS cluster, but `kubectl` must be explicitly configured.

### ğŸ”¹ Update kubeconfig

```bash
aws eks update-kubeconfig \
  --region us-east-1 \
  --name <EKS_CLUSTER_NAME>


Example:

aws eks update-kubeconfig \
  --region us-east-1 \
  --name ums-eks-cluster

ğŸ”¹ Verify current context
kubectl config current-context


Expected output:

arn:aws:eks:us-east-1:xxxx:cluster/ums-eks-cluster

âœ… Step 2: Verify EKS Cluster Health
ğŸ”¹ Check nodes
kubectl get nodes


Expected:

STATUS: Ready

ğŸ”¹ Check system pods
kubectl get pods -n kube-system


You should see pods like:

coredns

kube-proxy

aws-node

This confirms the cluster is healthy.

ğŸ“¦ Step 3: Prepare Kubernetes Manifests
3.1 Deployment Manifest (k8s/deployment.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ums-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ums
  template:
    metadata:
      labels:
        app: ums
    spec:
      containers:
        - name: ums-app
          image: <dockerhub-username>/user-management-service:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8000


ğŸ“Œ Notes:

Image is pulled from Docker Hub

replicas: 2 ensures high availability

3.2 Service Manifest (k8s/service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: ums-service
spec:
  type: LoadBalancer
  selector:
    app: ums
  ports:
    - port: 80
      targetPort: 8000


ğŸ“Œ Notes:

LoadBalancer is required for AWS EKS

AWS automatically provisions an ELB

ğŸš€ Step 4: Deploy Application to EKS

From the project root directory:

kubectl apply -f k8s/

ğŸ” Step 5: Verify Deployment
ğŸ”¹ Check deployments and pods
kubectl get deployments
kubectl get pods


Pods should be in:

STATUS: Running

ğŸ”¹ Check service
kubectl get svc


Output example:

ums-service   LoadBalancer   <CLUSTER-IP>   <EXTERNAL-IP>   80:xxxxx/TCP


âš ï¸ EXTERNAL-IP may show <pending> for 1â€“3 minutes.

ğŸŒ Step 6: Access Application in Browser
ğŸ”¹ Get LoadBalancer endpoint
kubectl get svc ums-service


Example:

EXTERNAL-IP: a1b2c3d4e5.elb.amazonaws.com

ğŸ”¹ Open in browser
http://a1b2c3d4e5.elb.amazonaws.com/health


ğŸ‰ The application is now live on AWS EKS.

ğŸ” Traffic Flow (Conceptual)
Browser
  â†“
AWS Load Balancer (ELB)
  â†“
Kubernetes Service
  â†“
Pod
  â†“
Container (FastAPI App)

ğŸ§ª Troubleshooting
ğŸ”¹ Pods not running
kubectl describe pod <pod-name>
kubectl logs <pod-name>

ğŸ”¹ LoadBalancer stuck in <pending>
kubectl describe svc ums-service


Possible reasons:

Missing subnet tags

AWS quota issues

ğŸ§¹ Cleanup (Important to Avoid Cost)

When practice is complete:

kubectl delete -f k8s/
terraform destroy

ğŸ—£ï¸ Interview-Ready Summary

â€œI provisioned AWS EKS using Terraform, configured kubectl access, deployed a containerized application using Kubernetes Deployments, and exposed it via a LoadBalancer service to access it through a public URL.â€